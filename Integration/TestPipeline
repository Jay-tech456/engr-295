# -*- coding: utf-8 -*-
"""DemoMar13.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Uors6DtisczTJp13SwW7cnZph4Mniu5O
"""

# Commented out IPython magic to ensure Python compatibility.
# # Cell 1: Install required packages
# %%capture
# !pip install -U openai-whisper
# !pip install torch
# !pip install spacy
# !pip install openai
# !pip install networkx
# !pip install matplotlib
# !pip install ffmpeg-python
# !pip install -U google-generativeai
# !pip install gradio
# !pip install keybert
# !pip install tabulate
# !python -m spacy download en_core_web_sm

# Cell 2: Import required libraries
import os
import io
import time
import math
import whisper
import torch
import spacy
import google.generativeai as genai
import networkx as nx
import matplotlib.pyplot as plt
from PIL import Image
from textwrap import fill
from tabulate import tabulate
from keybert import KeyBERT
import gradio as gr
import numpy as np
from threading import Thread
import queue

# Cell 3: Set up utility functions for text processing
def print_formatted_output(paragraph, keywords, width=80, columns=3):
    """Format paragraph and keywords for display"""
    # Format the paragraph
    formatted_paragraph = fill(paragraph, width=width)

    # Format the keywords
    num_keywords = len(keywords)
    rows = math.ceil(num_keywords / columns)
    keyword_table = [keywords[i:i+columns] for i in range(0, num_keywords, columns)]

    # Pad the last row if necessary
    if len(keyword_table[-1]) < columns:
        keyword_table[-1].extend([''] * (columns - len(keyword_table[-1])))

    formatted_keywords = tabulate(keyword_table, tablefmt="grid")

    # Combine and print
    print("Transcript:")
    print(formatted_paragraph)
    print("\nKeywords:")
    print(formatted_keywords)

# Function to capture matplotlib plots as images
def capture_plot_as_image():
    """Capture matplotlib plot as PIL image"""
    buf = io.BytesIO()
    plt.savefig(buf, format='png', bbox_inches='tight')
    plt.close()
    buf.seek(0)
    return Image.open(buf)

# Cell 4: Initialize models
# Load Whisper model (for speech recognition)
print("Loading Whisper model...")
model_whisper = whisper.load_model("base")

# Load spaCy model (for NLP tasks)
print("Loading spaCy model...")
nlp = spacy.load("en_core_web_sm")

# Load KeyBERT model (for keyword extraction)
print("Loading KeyBERT model...")
kw_model = KeyBERT()

# Cell 5: Set up Gemini API for generating explanations and concept maps
# Configure Google Gemini API
# Note: Replace with your actual API key
API_KEY = "AIzaSyAEs0Klhh4uPBi2dK_z-bmN5nD0klO17-w"  # Replace with your actual API key
os.environ["GOOGLE_API_KEY"] = API_KEY
genai.configure(api_key=API_KEY)
model = genai.GenerativeModel('gemini-1.5-flash')

# Cell 6: Functions to split transcriptions into minute segments
def split_transcription_by_minute(result):
    """
    Split Whisper transcription result into minute-long segments
    Returns list of dictionaries with 'minute' and 'text' keys
    """
    # Check if segments exist in result
    if "segments" not in result:
        # Fallback: if no segments, treat entire text as one segment
        return [{"minute": 0, "text": result["text"]}]

    segments = result["segments"]
    # Determine overall duration (in seconds)
    duration = segments[-1]["end"]
    num_minutes = math.ceil(duration / 60)

    minute_groups = {i: [] for i in range(num_minutes)}

    for seg in segments:
        # Find which minute group the segment belongs to
        minute_index = int(seg["start"] // 60)
        minute_groups[minute_index].append(seg["text"])

    # Join segment text for each minute
    minute_segments = []
    for minute, texts in minute_groups.items():
        segment_text = " ".join(texts)
        minute_segments.append({"minute": minute, "text": segment_text})

    return minute_segments

# Cell 7: Keyword extraction functions
def extract_physics_keywords(text):
    """
    Extract physics-related keywords from text using KeyBERT and filter with domain knowledge
    """
    # Physics domain terms for filtering
    physics_terms = [
        "energy", "force", "motion", "momentum", "acceleration", "velocity", "mass",
        "gravity", "electricity", "magnetism", "electromagnetism", "waves", "optics",
        "thermodynamics", "heat", "temperature", "pressure", "quantum", "relativity",
        "particle", "atom", "nucleus", "electron", "proton", "neutron", "field",
        "potential", "kinetic", "conservation", "entropy", "work", "power", "joule",
        "newton", "pascal", "watt", "coulomb", "ampere", "volt", "ohm", "resistance",
        "current", "circuit", "charge", "induction", "diffraction", "interference",
        "reflection", "refraction", "resonance", "frequency", "wavelength", "amplitude",
        "harmonic", "oscillation", "vibration", "radiation", "emission", "absorption"
    ]

    # Extract keywords using KeyBERT
    extracted_kws = kw_model.extract_keywords(
        text,
        keyphrase_ngram_range=(1, 2),
        stop_words='english',
        top_n=15
    )

    # Filter physics-related keywords and ensure we have at least some keywords
    physics_kws = []

    # First pass: exact matches with physics terms
    for kw, score in extracted_kws:
        if any(term in kw.lower() for term in physics_terms):
            physics_kws.append((kw, score))

    # If we don't have enough physics keywords, add general keywords
    if len(physics_kws) < 5:
        remaining_slots = 5 - len(physics_kws)
        general_kws = [(kw, score) for kw, score in extracted_kws if (kw, score) not in physics_kws][:remaining_slots]
        physics_kws.extend(general_kws)

    # Extract just the keywords (not scores)
    return [kw for kw, _ in physics_kws]

# Cell 8: Functions to generate explanations and concept maps
def get_keyword_explanation(keyword, education_level):
    """
    Generate a physics-focused explanation for a keyword based on education level
    """
    # Adjust prompt based on education level
    if education_level.lower() == "highschool":
        prompt = f"""Explain the physics concept of '{keyword}' in 2-3 sentences for high school students.
                    Use simple language and familiar examples. Focus on the core meaning without mathematical formulas."""
    elif education_level.lower() == "undergraduate":
        prompt = f"""Explain the physics concept of '{keyword}' in 3-4 sentences for undergraduate physics students.
                    Include key relationships to other physics concepts and 1-2 basic equations if relevant."""
    else:  # graduate
        prompt = f"""Explain the physics concept of '{keyword}' in 4-5 sentences for graduate physics students.
                    Include theoretical implications, modern applications, and precise terminology."""

    try:
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        return f"Error generating explanation: {e}"

def generate_concept_map(keywords, topic, education_level):
    """
    Generate a concept map showing relationships between physics keywords
    """
    if not keywords or len(keywords) < 2:
        # Need at least two keywords for a concept map
        return None

    # Adjust complexity based on education level
    if education_level.lower() == "highschool":
        complexity = "simple"
        num_concepts = min(5, len(keywords))
        cmap = "Blues"
    elif education_level.lower() == "undergraduate":
        complexity = "intermediate"
        num_concepts = min(7, len(keywords))
        cmap = "Greens"
    else:  # graduate
        complexity = "advanced"
        num_concepts = min(10, len(keywords))
        cmap = "Purples"

    # Select the most relevant keywords
    selected_keywords = keywords[:num_concepts]

    # Generate relationships between concepts using Gemini
    prompt = f"""
    Given these physics concepts: {', '.join(selected_keywords)}

    Generate a {complexity} concept map about "{topic}" for {education_level} students.

    For each concept, list 1-2 relationships to other concepts in the list.
    Format your response exactly like this example:

    CONCEPTS:
    - Concept1: Category1
    - Concept2: Category2

    RELATIONSHIPS:
    - From: Concept1, To: Concept2, Description: relates through X
    - From: Concept2, To: Concept3, Description: influences by Y
    """

    try:
        response = model.generate_content(prompt)

        # Parse the response into concepts and relationships
        concepts = []
        relationships = []
        sections = response.text.split("RELATIONSHIPS:")

        if len(sections) == 2:
            # Parse concepts
            concepts_section = sections[0].split("CONCEPTS:")[1].strip() if "CONCEPTS:" in sections[0] else ""
            for line in concepts_section.split("\n"):
                if line.strip().startswith("-") and ":" in line:
                    parts = line.strip().strip("-").split(":", 1)
                    if len(parts) == 2:
                        name = parts[0].strip()
                        category = parts[1].strip()
                        concepts.append({"name": name, "category": category})

            # Parse relationships
            relationships_section = sections[1].strip()
            for line in relationships_section.split("\n"):
                if line.strip().startswith("-") and "From:" in line and "To:" in line:
                    # Extract from, to, and description
                    parts = line.strip("-").strip().split(",")
                    if len(parts) >= 3:
                        from_part = parts[0].strip().replace("From:", "").strip()
                        to_part = parts[1].strip().replace("To:", "").strip()
                        desc_part = ",".join(parts[2:]).strip().replace("Description:", "").strip()

                        relationships.append({
                            "from": from_part,
                            "to": to_part,
                            "description": desc_part
                        })

        # Create a fallback if parsing failed
        if not concepts or not relationships:
            concepts = [{"name": kw, "category": "concept"} for kw in selected_keywords]
            relationships = []
            for i in range(len(selected_keywords) - 1):
                relationships.append({
                    "from": selected_keywords[i],
                    "to": selected_keywords[i+1],
                    "description": "relates to"
                })

        # Create graph
        G = nx.Graph()

        # Add nodes
        for concept in concepts:
            G.add_node(concept["name"], group=concept["category"])

        # Add edges
        for rel in relationships:
            if rel["from"] in G.nodes() and rel["to"] in G.nodes():
                G.add_edge(rel["from"], rel["to"], relation=rel["description"])

        # Create the plot
        plt.figure(figsize=(12, 8))

        # Use spring layout for the graph
        pos = nx.spring_layout(G, k=0.5, iterations=50, seed=42)

        # Get node groups for coloring
        groups = nx.get_node_attributes(G, 'group')
        unique_groups = set(groups.values())
        color_map = plt.cm.get_cmap(cmap, len(unique_groups) + 1)

        # Create color mapping
        color_mapping = {group: color_map(i) for i, group in enumerate(unique_groups)}
        node_colors = [color_mapping.get(groups.get(node, "other"), "lightgray") for node in G.nodes()]

        # Size nodes based on degree centrality
        centrality = nx.degree_centrality(G)
        node_sizes = [3000 * (0.5 + centrality.get(node, 0)) for node in G.nodes()]

        # Draw the graph
        nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=node_sizes, alpha=0.8)
        nx.draw_networkx_edges(G, pos, edge_color="gray", width=1.5, alpha=0.7)

        # Draw labels
        nx.draw_networkx_labels(G, pos, font_size=10, font_weight="bold")

        # Add edge labels (relations)
        edge_labels = nx.get_edge_attributes(G, 'relation')
        nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8)

        plt.title(f"Physics Concept Map: {topic} ({education_level})", fontsize=14)
        plt.axis('off')
        plt.tight_layout()

        # Capture and return the plot as an image
        return capture_plot_as_image()

    except Exception as e:
        print(f"Error generating concept map: {e}")
        return None

# Cell 9: Define transcriber class for real-time processing
class AsyncTranscriber:
    """
    Handles asynchronous transcription of audio with real-time updates
    """
    def __init__(self, audio_path, model, update_interval=5):
        self.audio_path = audio_path
        self.model = model
        self.update_interval = update_interval  # seconds
        self.result_queue = queue.Queue()
        self.full_result = None
        self.segments = []
        self.is_running = False

    def start(self):
        """Start the transcription process in a separate thread"""
        self.is_running = True
        self.thread = Thread(target=self._transcribe)
        self.thread.daemon = True
        self.thread.start()

    def _transcribe(self):
        """Perform the transcription and add segments to the queue"""
        try:
            # Run Whisper transcription
            self.full_result = self.model.transcribe(self.audio_path, verbose=False)

            # Process and queue the segments
            segments = self.full_result.get("segments", [])
            self.segments = split_transcription_by_minute(self.full_result)

            # Put entire result in queue for UI to handle
            self.result_queue.put({"type": "complete", "result": self.full_result, "segments": self.segments})

        except Exception as e:
            self.result_queue.put({"type": "error", "error": str(e)})
        finally:
            self.is_running = False

    def get_updates(self, timeout=0.1):
        """Get any available updates from the queue"""
        try:
            return self.result_queue.get(timeout=timeout)
        except queue.Empty:
            return None

# Cell 10: Gradio interface functions
def upload_audio(audio_file):
    """Initialize when audio is uploaded, before processing"""
    if audio_file is None:
        return "No audio provided", [], None, gr.update(maximum=0, value=0)

    # Return empty values but provide audio for playback
    return "Audio ready for processing. Click 'Process Audio' to begin.", [], audio_file, gr.update(maximum=0, value=0)

def process_audio(audio_file, progress=gr.Progress()):
    """Process audio file and prepare for transcription and analysis"""
    if audio_file is None:
        return "No audio file provided", [], None, gr.update(maximum=0, value=0)

    progress(0, desc="Starting transcription...")

    # Create and start the transcriber
    transcriber = AsyncTranscriber(audio_file, model_whisper)
    transcriber.start()

    # Initial placeholder for the UI
    yield "Transcription in progress...", [], audio_file, gr.update(maximum=0, value=0)

    # Poll for transcription updates
    timeout_counter = 0
    while transcriber.is_running and timeout_counter < 180:  # 3 minute timeout
        update = transcriber.get_updates(timeout=0.1)
        if update:
            if update["type"] == "complete":
                # Get full transcription
                full_text = update["result"]["text"]
                segments = update["segments"]

                # Determine number of segments for the slider
                max_segment = len(segments) - 1 if segments else 0

                # Return the full data
                yield full_text, segments, audio_file, gr.update(maximum=max_segment, value=0)
                break

            elif update["type"] == "error":
                yield f"Error during transcription: {update['error']}", [], None, gr.update(maximum=0, value=0)
                break

        # Update progress
        progress(min(timeout_counter/60, 0.95), desc="Transcribing audio...")
        timeout_counter += 1
        time.sleep(1)

    # If we timed out
    if timeout_counter >= 180 and transcriber.is_running:
        yield "Transcription timed out after 3 minutes", [], audio_file, gr.update(maximum=0, value=0)

def load_segment(segment_index, segments):
    """Load a specific segment's transcript and extract physics keywords"""
    if not segments or segment_index is None:
        return "No segments available", gr.update(choices=[], value=[])

    # Find the segment with the given index
    for seg in segments:
        if seg["minute"] == segment_index:
            transcript = seg["text"]

            # Extract physics-focused keywords
            keywords = extract_physics_keywords(transcript)

            # Update the checkbox group with the new keywords
            keyword_update = gr.update(choices=keywords, value=[])
            return transcript, keyword_update

    # If no segment found
    return "Segment not found", gr.update(choices=[], value=[])

def get_keyword_details(selected_keyword, education_level):
    """Get educational level-appropriate explanations for selected keywords"""
    if not selected_keyword:
        return "No keyword selected. Please select a keyword to get its explanation."

    # Get physics-focused explanation based on education level
    explanation = get_keyword_explanation(selected_keyword, education_level)
    return explanation

def create_mindmap(segment_text, education_level):
    """Generate a concept map for the segment based on education level"""
    if not segment_text or segment_text == "No segments available" or segment_text == "Segment not found":
        return None

    # Extract keywords from the segment
    keywords = extract_physics_keywords(segment_text)

    # Determine topic from the segment
    # Here we use a simple approach to find the most common physics term
    physics_terms = ["thermodynamics", "mechanics", "electromagnetism", "optics",
                     "quantum mechanics", "relativity", "nuclear physics", "waves"]

    # Default topic
    topic = "Physics Concepts"

    # Try to determine a more specific topic
    for term in physics_terms:
        if term in segment_text.lower():
            topic = term.title()
            break

    # Generate and return the concept map
    mindmap_img = generate_concept_map(keywords, topic, education_level)
    return mindmap_img

# Cell 11: Gradio UI definition
def create_physics_analyzer_ui():
    """Create the Gradio interface for the Physics Lecture Analyzer"""
    with gr.Blocks(theme=gr.themes.Soft()) as demo:
        gr.Markdown("# Physics Lecture Analyzer")
        gr.Markdown("Upload an audio file of a physics lecture for real-time analysis and visualization")

        with gr.Row():
            with gr.Column(scale=1):
                # Input section
                audio_input = gr.Audio(type="filepath", label="Upload Audio File")
                audio_player = gr.Audio(label="Audio Playback")
                process_button = gr.Button("Process Audio", variant="primary")

                # Education level selection
                education_radio = gr.Radio(
                    choices=["Highschool", "Undergraduate", "Graduate"],
                    label="Education Level",
                    value="Undergraduate"
                )

            with gr.Column(scale=2):
                # Transcription output
                transcript_output = gr.Textbox(
                    label="Full Transcription",
                    lines=8,
                    placeholder="The full lecture transcript will appear here after processing..."
                )

                # Segment selection and analysis
                gr.Markdown("### Minute-by-Minute Analysis")
                minute_slider = gr.Slider(
                    minimum=0,
                    maximum=0,  # Will be updated after processing
                    step=1,
                    label="Select Minute Segment",
                    value=0
                )

                with gr.Row():
                    with gr.Column(scale=1):
                        segment_transcript = gr.Textbox(
                            label="Segment Transcript",
                            lines=4,
                            placeholder="The selected segment transcript will appear here..."
                        )
                        segment_keywords = gr.CheckboxGroup(
                            label="Extracted Physics Keywords",
                            choices=[]
                        )
                        get_keyword_button = gr.Button("Get Keyword Details")
                        keyword_output = gr.Textbox(
                            label="Keyword Explanation",
                            lines=5,
                            placeholder="Select a keyword and click 'Get Keyword Details'..."
                        )

                    with gr.Column(scale=1):
                        get_mindmap_button = gr.Button("Generate Concept Map")
                        mindmap_output = gr.Image(
                            label="Physics Concept Map",
                            height=400
                        )

        # Hidden state to store segments data
        segments_state = gr.State([])

        # Wire up the event handlers

        # Initialize when audio is uploaded
        audio_input.change(
            fn=upload_audio,
            inputs=[audio_input],
            outputs=[transcript_output, segments_state, audio_player, minute_slider]
        )

        # Process audio when button is clicked
        process_button.click(
            fn=process_audio,
            inputs=[audio_input],
            outputs=[transcript_output, segments_state, audio_player, minute_slider]
        )

        # Update segment info when slider changes
        minute_slider.change(
            fn=load_segment,
            inputs=[minute_slider, segments_state],
            outputs=[segment_transcript, segment_keywords]
        )

        # Get keyword details when button is clicked
        get_keyword_button.click(
            fn=get_keyword_details,
            inputs=[segment_keywords, education_radio],
            outputs=[keyword_output]
        )

        # Generate mind map when button is clicked
        get_mindmap_button.click(
            fn=create_mindmap,
            inputs=[segment_transcript, education_radio],
            outputs=[mindmap_output]
        )

        # Help text at the bottom
        gr.Markdown("""
        ### How to Use:
        1. Upload an audio file of a physics lecture
        2. Click 'Process Audio' to begin analysis
        3. Use the slider to navigate through minute segments
        4. Select keywords to get detailed explanations
        5. Generate concept maps to visualize relationships

        *Note: Processing may take a few moments depending on the audio length*
        """)

    return demo

# Cell 12: Launch the application
if __name__ == "__main__":
    # Replace this key with your actual API key
    API_KEY = "AIzaSyAEs0Klhh4uPBi2dK_z-bmN5nD0klO17-w"
    os.environ["GOOGLE_API_KEY"] = API_KEY
    genai.configure(api_key=API_KEY)

    demo = create_physics_analyzer_ui()
    demo.launch(share=True, debug=True)